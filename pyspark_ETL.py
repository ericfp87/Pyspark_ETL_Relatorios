# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1khrfGRI0y7dzHsHgTub_ZL1fe5fL2Mtr
"""

!pip install pyspark

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, unix_timestamp, from_unixtime, regexp_replace
from pyspark.sql.types import DecimalType

# Cria uma sessão Spark
spark = SparkSession.builder.master("local[*]").getOrCreate()
spark.conf.set("spark.sql.decimal", ",")


df_poa_temp = spark.read.csv("CORVINA/POA_TEMP_CORVINA.csv",sep=";", header=True, inferSchema=True)
df_p = spark.read.csv("CORVINA/P_CORVINA.csv",sep=";", header=True, inferSchema=True)

df = df_poa_temp.join(df_p, "Data", "inner")

df.withColumn("Data", from_unixtime(unix_timestamp(col("Data"), "dd/MM/yyyy HH:mm:ss"), "d/M/yy H:mm"))


colunas_float = ["Irr(W/m²)", "Tm(°C)", "Ta(°C)", "P(W)"]
for coluna_float in colunas_float:
    df = df.withColumn(coluna_float, regexp_replace(col(coluna_float), ",", ".").cast(DecimalType(10, 5)))


df = df.select(["Data", "Irr(W/m²)", "Tm(°C)", "Ta(°C)", "P(W)"])

df.show()

df.coalesce(1).write.csv("CORVINA/relatorio-corvina.csv", header=True, mode="overwrite")

df_poa_temp = spark.read.csv("CJ1/POA_TEMP_CJ1.csv",sep=";", header=True, inferSchema=True)
df_p = spark.read.csv("CJ1/P_CJ1.csv",sep=";", header=True, inferSchema=True)

df = df_poa_temp.join(df_p, "Data", "inner")

df.withColumn("Data", from_unixtime(unix_timestamp(col("Data"), "dd/MM/yyyy HH:mm:ss"), "d/M/yy H:mm"))


colunas_float = ["Irr(W/m²)", "Tm(°C)", "Ta(°C)", "P(W)"]
for coluna_float in colunas_float:
    df = df.withColumn(coluna_float, regexp_replace(col(coluna_float), ",", ".").cast(DecimalType(10, 5)))


df = df.select(["Data", "Irr(W/m²)", "Tm(°C)", "Ta(°C)", "P(W)"])

df.show()

df.coalesce(1).write.csv("CJ1/relatorio-cj1.csv", header=True, mode="overwrite")

df_poa_temp = spark.read.csv("JQ1/POA_TEMP_JQ1.csv",sep=";", header=True, inferSchema=True)
df_p = spark.read.csv("JQ1/P_JQ1.csv",sep=";", header=True, inferSchema=True)

df = df_poa_temp.join(df_p, "Data", "inner")

df.withColumn("Data", from_unixtime(unix_timestamp(col("Data"), "dd/MM/yyyy HH:mm:ss"), "d/M/yy H:mm"))


colunas_float = ["Irr(W/m²)", "Tm(°C)", "Ta(°C)", "P(W)"]
for coluna_float in colunas_float:
    df = df.withColumn(coluna_float, regexp_replace(col(coluna_float), ",", ".").cast(DecimalType(10, 5)))


df = df.select(["Data", "Irr(W/m²)", "Tm(°C)", "Ta(°C)", "P(W)"])

df.show()

df.coalesce(1).write.csv("JQ1/relatorio-jq1.csv", header=True, mode="overwrite")

df_poa_temp = spark.read.csv("MB2/POA_TEMP_MB2.csv",sep=";", header=True, inferSchema=True)
df_p = spark.read.csv("MB2/P_MB2.csv",sep=";", header=True, inferSchema=True)

df = df_poa_temp.join(df_p, "Data", "inner")

df.withColumn("Data", from_unixtime(unix_timestamp(col("Data"), "dd/MM/yyyy HH:mm:ss"), "d/M/yy H:mm"))


colunas_float = ["Irr(W/m²)", "Tm(°C)", "Ta(°C)", "P(W)"]
for coluna_float in colunas_float:
    df = df.withColumn(coluna_float, regexp_replace(col(coluna_float), ",", ".").cast(DecimalType(10, 5)))


df = df.select(["Data", "Irr(W/m²)", "Tm(°C)", "Ta(°C)", "P(W)"])

df.show()

df.coalesce(1).write.csv("MB2/relatorio-mb2.csv", header=True, mode="overwrite")

df_poa_temp = spark.read.csv("SL1/POA_TEMP_SL1.csv",sep=";", header=True, inferSchema=True)
df_p = spark.read.csv("SL1/P_SL1.csv",sep=";", header=True, inferSchema=True)

df = df_poa_temp.join(df_p, "Data", "inner")

df.withColumn("Data", from_unixtime(unix_timestamp(col("Data"), "dd/MM/yyyy HH:mm:ss"), "d/M/yy H:mm"))


colunas_float = ["Irr(W/m²)", "Tm(°C)", "Ta(°C)", "P(W)"]
for coluna_float in colunas_float:
    df = df.withColumn(coluna_float, regexp_replace(col(coluna_float), ",", ".").cast(DecimalType(10, 5)))


df = df.select(["Data", "Irr(W/m²)", "Tm(°C)", "Ta(°C)", "P(W)"])

df.show()

df.coalesce(1).write.csv("SL1/relatorio-SL1.csv", header=True, mode="overwrite")

df_poa_temp = spark.read.csv("TB2/POA_TEMP_TB2.csv",sep=";", header=True, inferSchema=True)
df_p = spark.read.csv("TB2/P_TB2.csv",sep=";", header=True, inferSchema=True)

df = df_poa_temp.join(df_p, "Data", "inner")

df.withColumn("Data", from_unixtime(unix_timestamp(col("Data"), "dd/MM/yyyy HH:mm:ss"), "d/M/yy H:mm"))


colunas_float = ["Irr(W/m²)", "Tm(°C)", "Ta(°C)", "P(W)"]
for coluna_float in colunas_float:
    df = df.withColumn(coluna_float, regexp_replace(col(coluna_float), ",", ".").cast(DecimalType(10, 5)))


df = df.select(["Data", "Irr(W/m²)", "Tm(°C)", "Ta(°C)", "P(W)"])

df.show()

df.coalesce(1).write.csv("TB2/relatorio-TB2.csv", header=True, mode="overwrite")

df_poa_temp = spark.read.csv("TB3/POA_TEMP_TB3.csv",sep=";", header=True, inferSchema=True)
df_p = spark.read.csv("TB3/P_TB3.csv",sep=";", header=True, inferSchema=True)

df = df_poa_temp.join(df_p, "Data", "inner")

df.withColumn("Data", from_unixtime(unix_timestamp(col("Data"), "dd/MM/yyyy HH:mm:ss"), "d/M/yy H:mm"))


colunas_float = ["Irr(W/m²)", "Tm(°C)", "Ta(°C)", "P(W)"]
for coluna_float in colunas_float:
    df = df.withColumn(coluna_float, regexp_replace(col(coluna_float), ",", ".").cast(DecimalType(10, 5)))


df = df.select(["Data", "Irr(W/m²)", "Tm(°C)", "Ta(°C)", "P(W)"])

df.show()

df.coalesce(1).write.csv("TB3/relatorio-TB3.csv", header=True, mode="overwrite")

df_poa_temp = spark.read.csv("MB1/POA_TEMP_MB1.csv",sep=";", header=True, inferSchema=True)
df_p = spark.read.csv("MB1/P_MB1.csv",sep=";", header=True, inferSchema=True)

df = df_poa_temp.join(df_p, "Data", "inner")

df.withColumn("Data", from_unixtime(unix_timestamp(col("Data"), "dd/MM/yyyy HH:mm:ss"), "d/M/yy H:mm"))


colunas_float = ["Irr(W/m²)", "Tm(°C)", "Ta(°C)", "P(W)"]
for coluna_float in colunas_float:
    df = df.withColumn(coluna_float, regexp_replace(col(coluna_float), ",", ".").cast(DecimalType(10, 5)))


df = df.select(["Data", "Irr(W/m²)", "Tm(°C)", "Ta(°C)", "P(W)"])

df.show()

df.coalesce(1).write.csv("MB1/relatorio-MB1.csv", header=True, mode="overwrite")